# 来源：https://zhuanlan.zhihu.com/p/40775756
1.数据缺失的原因：
    -无意的：信息被遗漏，比如由于工作人员的疏忽，忘记而缺失；或者由于数据采集器等故障等原因造成的缺失，
           比如系统实时性要求较高的时候，机器来不及判断和决策而造成缺失；
    -有意的：有些数据集在特征描述中会规定将缺失值也作为一种特征值，这时候缺失值就可以看作是一种特殊的特征值；
    -不存在：有些特征属性根本就是不存在的，比如一个未婚者的配偶名字就没法填写，再如一个孩子的收入状况也无法填写；
2.数据缺失的类型：
    -完全随机缺失（missing completely at random,MCAR）：指的是数据的缺失是完全随机的，不依赖于任何不完全变量或完全变量，不影响样本的无偏性，如家庭地址缺失；
    -随机缺失(missing at random,MAR)：指的是数据的缺失不是完全随机的，即该类数据的缺失依赖于其他完全变量，如财务数据缺失情况与企业的大小有关；
    -非随机缺失(missing not at random,MNAR)：指的是数据的缺失与不完全变量自身的取值有关，如高收入人群不原意提供家庭收入；
    （对于随机缺失和非随机缺失，直接删除记录是不合适的，原因上面已经给出。随机缺失可以通过已知变量对缺失值进行估计，而非随机缺失的非随机性还没有很好的解决办法。）
3.数据缺失的处理方法：【删除记录 or 数据填补 or 不处理】
    -1.删除：
        优点：简单粗暴
        缺点：
            -牺牲了大量的数据，通过减少历史数据换取完整的信息，这样可能丢失了很多隐藏的重要信息；
            -当缺失数据比例较大时，特别是缺失数据非随机分布时，直接删除可能会导致数据发生偏离，比如原本的正态分布变为非正太；
    -2.数据填补：【替换缺失值 or 拟合缺失值 or 虚拟变量】
        * 替换缺失值：【均值插补 or 热卡填补 or K最近距离邻法（K-means clustering）】
            均值插补：
                -对于定类数据：使用 众数（mode）填补，比如一个学校的男生和女生的数量，男生500人，女生50人，那么对于其余的缺失值我们会用人数较多的男生来填补。
                -对于定量（定比）数据：使用平均数（mean）或中位数（median）填补，比如一个班级学生的身高特征，对于一些同学缺失的身高值就可以使用全班同学身高的平均值或中位数来填补。
                                    一般如果特征分布为正太分布时，使用平均值效果比较好，而当分布由于异常值存在而不是正太分布的情况下，使用中位数效果比较好。
                （此方法虽然简单，但是不够精准，可能会引入噪声，或者会改变特征原有的分布。）
            热卡填补：
                热卡填充法是在完整数据中找到一个与它最相似的对象，然后用这个相似对象的值来进行填充。通常会找到超出一个的相似对象，在所有匹配对象中没有最好的，而是从中随机的挑选一个作为填充值。
                这个问题关键是不同的问题可能会选用不同的标准来对相似进行判定，以及如何制定这个判定标准。该方法概念上很简单，且利用了数据间的关系来进行空值估计，但缺点在于难以定义相似标准，主观因素较多。
            K最近距离邻法（K-means clustering）：
                通过K均值的聚类方法将所有样本进行聚类划分，然后再通过划分的种类的均值对各自类中的缺失值进行填补。归其本质还是通过找相似来填补缺失值。
                注：缺失值填补的准确性就要看聚类结果的好坏了，而聚类结果的可变性很大，通常与初始选择点有关，并且在下图中可看到单独的每一类中特征值也有很大的差别，因此使用时要慎重。
        * 拟合缺失值：【回归预测 or 极大似然估计 or 多重插补 or 随机森林 or 虚拟变量】
            注：如果其它特征变量与缺失变量无关，则预测的结果毫无意义。如果预测结果相当准确，则又说明这个变量完全没有必要进行预测，因为这必然是与特征变量间存在重复信息。
                一般情况下，会介于两者之间效果为最好，若强行填补缺失值之后引入了自相关，这会给后续分析造成障碍。
            回归预测：
                缺失值是连续的，即定量的类型，才可以使用回归来预测。
            极大似然估计（Maximum likelyhood）：
                在缺失类型为随机缺失的条件下，假设模型对于完整的样本是正确的，那么通过观测数据的边际分布可以对未知参数进行极大似然估计（Little and Rubin）。
                这种方法也被称为忽略缺失值的极大似然估计，对于极大似然的参数估计实际中常采用的计算方法是期望值最大化(Expectation Maximization，EM）。
                该方法比删除个案和单值插补更有吸引力，它一个重要前提：适用于大样本。有效样本的数量足够以保证ML估计值是渐近无偏的并服从正态分布。
                但是这种方法可能会陷入局部极值，收敛速度也不是很快，并且计算很复杂，且仅限于线性模型。
            多重插补（Mutiple imputation）：
                多值插补的思想来源于贝叶斯估计，认为待插补的值是随机的，它的值来自于已观测到的值。具体实践上通常是估计出待插补的值，然后再加上不同的噪声，形成多组可选插补值。根据某种选择依据，选取最合适的插补值。
                我们看到，以上提出的拟合和替换方法都是单一的插补方法，而多重插补弥补了单一插补的缺陷，它并没有试图去通过模拟值去估计每个缺失值，而是提出缺失数据值的一个随即样本（这些样本可以是不同的模型拟合结果的组合）。
                这种程序的实施恰当地反映了由于缺失值引起的不确定性，使得统计推断有效。多重插补推断可以分为以下3个步骤：
                    > 为每个缺失值产生一套可能的插补值，这些值反映了无响应模型的不确定性；
                    > 每个插补数据集合都用针对完整数据集的统计方法进行统计分析；
                    > 对来自各个插补数据集的结果，根据评分函数进行选择，产生最终的插补值；
            随机森林：
        * 虚拟变量：
            虚拟变量其实就是缺失值的一种衍生变量。具体做法是通过判断特征值是否有缺失值来定义一个新的二分类变量。比如，特征为A含有缺失值，我们衍生出一个新的特征B，如果A中特征值有缺失，那么相应的B中的值为1，如果A中特征值没有缺失，那么相应的B中的值为0。
    -3.不处理：
        补齐处理只是将未知值补以我们的主观估计值，不一定完全符合客观事实，在对不完备信息进行补齐处理的同时，我们或多或少地改变了原始的信息系统。而且，对空值不正确的填充往往将新的噪声引入数据中，使挖掘任务产生错误的结果。
        因此，在许多情况下，我们还是希望在保持原始信息不发生变化的前提下对信息系统进行处理。
        部分模型本身可以应对具有缺失值的数据，比如XGBOOST等；


