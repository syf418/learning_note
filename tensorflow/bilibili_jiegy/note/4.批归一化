1.批归一化：
    每层的激活值都做归一化
2.Dropout:
    - 防止过拟合
        - 参数太多，记住样本（每层激活的数量比较多），不能泛化
    - AlphaDropout:
        - 均值和方差不变
        - 归一化性质也不变
3.激活函数：
    - MaxOut
    - selu: 自带归一化功能
4.DNN起始损失不下降：
    - 参数众多，训练不充分
    - 梯度消失:
        - 批归一化可以缓解梯度消失
        - selu因为自带归一化，故也可以缓解梯度消失